\chapter{Base de Probablite} % (fold)
\label{cha:base_de_probablite}

% chapter base_de_probablite (end)
\section{Espase Probabilisé} % (fold)
\label{sec:course_1}

% section course_1 (end)
Soit $\Omega$ est \textsc{Univers} (est random ensemble).

\begin{definition}[$\sigma$ - algebra]
	
	La famille des ensembles $\mathcal{A}$ s'appelle \textsc{$\sigma$-Algebra} si:
	\begin{enumerate}
		\item $\Omega\in \mathcal{A}$
		\item Pour tout $A\in \mathcal{A}$, $A^c\in \mathcal{A}$ ($A^C=\bar{A}$)
		\item Si $\{A_k\}^\infty_{k=1}\in \mathcal{A}:\ \cup^\infty_{k=1}A_k\in\mathcal{A}$
	\end{enumerate} 
\end{definition}


\begin{definition}[Probabilité]
	\leavevmode
	\begin{enumerate}
		\item $\pro (\Omega)=1$
		\item Si $\{A_k\}_k^\infty$ - disjoint (pour tout $i\neq j:\ A_i\cup A_j = \emptyset$):
	\end{enumerate}
		$$\pro (\cup^\infty_{k=1}A_k)=\sum^\infty_{k=1}\pro (A_k)$$

\end{definition}

Espasce probabilisable $( \underbrace{\Omega}_\text{univers},\ 
\underbrace{ \mathcal{A} }_\text{tribu} )$.

Espase probabillisé $(\Omega,\ \mathcal{A},\ \pro )$.

\begin{definition}
\textsc{Variable Aléatoire} (random variable) est fonction measurable $X$: $$X:\Omega\rightarrow\mathbb{R}$$
\end{definition}

\begin{diagram}
\Omega & \rTo^X & \mathbb{R} \\
\dTo<{\sigma\text{-algebre}} &  & \dTo>{\sigma\text{-algebre}} \\ 
\mathcal{A} &  &  \mathcal{B}\text{ (Borel)}
\end{diagram}

Soit $\Omega$ un ensemble. Soit $\mathcal{F}$ un famille d'ensembles de $\Omega$, qui n'est pas forcément une $\sigma$-algèbre.

\begin{definition}
	On appelle $\sigma$-algèbre engendrée par $\mathcal{F}$, dénotée $\sigma(\mathcal{F})$ la plus petite $\sigma$-algèbre que contient $\mathcal{F}$.
\end{definition}

\begin{definition}
	Borel ($\mathcal{B}$) est la $\sigma$-algebre engendrée par les intervalles ouvertes de $\mathbb{R}$ c'est-â-dire de la forme $(a,\ b),\ |a|,\ |b| < \infty$ (famile $\mathcal{F}_0$).
\end{definition}

\begin{remark}
On dit Borel ($\mathcal{B}$) est aussi $\sigma$-algebre engendrée par des intervalles de la forme $(-\infty,\ |a|],\ |a|<\infty$ (famile $\mathcal{F}_{FN}$).
	$\sigma(\mathcal{F}_0)=\sigma(\mathcal{F}_{FN})$
\end{remark}

\begin{proposition}
	Pour verifier la measurable il suffit de la tester sur une famille qui engendrée la $\sigma$-algèbre de Borele.
\end{proposition}

\emph{Exercice}. (simple mais important)\\
Soit $\Omega$ un ensemble. $\mathcal{P}= \{P_1,\ P_2,\,...\,,\ P_k\}$ est une partition finit de $\Omega$, c'est-â-dire $\cup_{j=1}^k P_j=\Omega$ et $P_\alpha\cap P_\beta=\varnothing$.

\begin{enumerate}
	\item Trouve $\sigma(\mathcal{P})$. Réponse:\\
	$\sigma(\mathcal{P})$ contient tout reunion d'éléments $\mathcal{P}$.\\
(En partiqulier si $A\in\sigma(\mathcal{P})$: $A=\cup_{k=1}^l P_{i_k}$)
	\item Trouve comment sont faites les v.a. par rapport â $\sigma(\mathcal{P})$. Réponse:\\
	Consider $\Omega=\mathbb{R}$. $X(\omega)=\alpha$. $\alpha$ est l'image $\omega$. Le point  $\alpha$ est aussi un ensemble, qu'on denote $\{\alpha\}$: "singlitore" qui est un borelien. Car $X$ est measurable par rapport â $\sigma(\mathcal{P})$, $X^{-1}(\{\alpha\})=\cup P_{i_k}$.
	
	Une function measurable pour rapport à $\sigma(\mathcal{P})$ est constante par morceaux sr les éléments de la partition. 
	
	On replace $X$ avec autre object qui "approxime" $X$ est measurable par rapport â $\sigma(\mathcal{P})$.
\end{enumerate}

Espace probabilisé $(\Omega,\ \mathcal{A},\ \pro )$. $X:\Omega\rightarrow\underbrace{\mathbb{R}}_\text{Borel}$,  $X$ est v.a. 

Loi de $X$ on définir un mesure de probabilite sur ($\mathbb{R},\ \mathcal{B}$) de la manière suivante si $B\in\mathcal{B}$: $P_X(B)=\pro (X^{-1}(B))$.

On appelle $P_X$ de \textsc{La Loi de $X$}.

$X:\Omega\rightarrow\mathbb{R}$ On pourra écrite $X$ de la maniere suivante: $X(\omega)=\sum_{k=1}^\infty x_k \ind _{A_k}(\omega)$, $A_k=\{\omega\, |\, X(\omega)\in A_k\}$. Calculer $P_X$ (la loi de $X$):

Si $B\in\mathcal{F}$, $P_X(B) = \pro (X^{-1}(B))$.

On appelle $D$ l'ensemble valeur de $X$: $D=\{x_1,\ x_2,\, ...\,x_k\, ...\}$.

$P_X(B)=P_X(B\cap D)=P_X(B\cap \cup_{k=1}^\infty\{x_k\})=P_X(\cup_{k=1}^\infty(B\cap \{x_k\}))=\sum_{k=1}^\infty P_X(B\cap \{x_k\})=\sum_{k=1}^\infty \pro (X=x_k)\delta_{\{x_k\}}(B)=\sum_{k=1}^\infty p_k \delta_{\{x_k\}}(B)$

$$\delta_a(B)=\left\{
\begin{array}{rl}
	1 & \mbox{ si } a\in B \\ 
	0 & \mbox{ si } a\notin B
\end{array}\right.$$

On introduit la measure de Dirrac:

$$\left. \begin{array}{rrl}
	X & = &\sum\limits_{k=1}^\infty x_k \ind _{A_k}\\ 
	P_x & = & \sum\limits_{k=1}^\infty p_k \delta_{\{x_k\}}\\
	P_x & = & \pro (A_k)
\end{array}\right\}\text{ v.a. discrete}$$

Exemple. (v.a. discrete)
\begin{enumerate}
	
	\item $B(n,\ p)$ binomiale\\
Valeurs: $X=\{0,\ 1,\,...\, n\}$.
$$P_k=\pro (X=k)=C_n^k p^k (1-p)^{n-k},\ i\in\{0,\, ...\,, n\}$$
	\item Poisson $P(\lambda)$. Valeurs $X=\{0,\ 1,\ 2,\, ...\,\}$ - dénombrable.
	
	$$P_X(\{k\})=\pro (X=k)=\frac{e^{-\lambda}\lambda^k}{k!}$$ 
\end{enumerate}

\begin{rappel}
	$X:Ω\rightarrow \R$ est une \texttt{variable aléatoire}
	
	$\E(X)=∫_ΩX(ω)\dd{\pro(ω)}$ --- esperance,
	$\V(X)=\E[[X-\E(X)]²]$ --- variance.
	
	Supposons $\E(X)= 0\Rightarrow \V(X)=\E(X²)$.
	
	$g(t)=t²$, $g◦X=X^2$. Si $g\circ X=X\circ g$, $g$ --- identité.
\end{rappel}

\begin{rappel}
	\texttt{Variable aléatoire} à valeurs réelle: $X:Ω\rightarrow \R$. \texttt{Loi de} $X$: une measure de probabilité sur $\R$ $P_X(B)=\pro(X\dmo(B)),\ B\in\mathcal{B}$.
	
\end{rappel}

\begin{theorem}
	Soit $X:Ω\rightarrow \R$ une v.a. sur l'espace probabilisé $(Ω,\mathcal{A}, \pro)$ et soit $g:\R\rightarrow \R$ une fonction mesurable. si l'intégrale $∫_Ω g\circ X\dd{\pro}$ existe on a
	$$∫_Ωg(X(ω))\dd{\pro(ω)}=∫_\R g(t)\dd{P_X(t)}$$
\end{theorem}

\begin{examplebox}
	Supposons que $X$ est discrète...
	P1/2
\end{examplebox}

\begin{definition}
	$X:Ω\rightarrow \R$ v.a.
	$$F(t):=\pro(X≤t)=\pro(ω:\ X(ω)≤t)=P_X((-∞,t])$$
\end{definition}

\begin{proposition}
	Toute fonction de repartition $F$ vérifie les propriétés suivantes:
	\begin{enumerate}
		\item $F$ est non-negative et croissante.
		\item $F$ est continue à droite.
		\item $F$ est discontinue dans plus un nombre dénombrée de point.
		\item $\left\{\begin{array}{l}\lim\limits_{t\to +∞}F(t)=1\\ \lim\limits_{t\to -∞}F(t)=0\end{array}\right.$.
	\end{enumerate}
\end{proposition}

\subsection{Rappel de Th. de la measure} % (fold)
\label{sub:rappel_de_th_de_la_measure}
Soit $F$ une fonction croissante réelle positive (en particulier $F$ est la fonction de repartition d'une v.a.).

Ou définit une fonction d'ensemble sur $\R$:
\[\tilde F((a,b])=F(b)-F(a)\ \label{eq:leb-stilt-measure} \tag{*}\]
Il y a un théorème de théorie de la measure que dit que la fonction d'ensemble $\tilde F$ défini sur la famille $\{[a,b]\}$ peut s'étendre à une measure sur la $σ$-algebra engendrée par cette famille ($\mathcal B$ --- Borel) et la restriction de cette mesure sur la famille $\{[a,b]\}$ vérifie l'égalité \eqref{eq:leb-stilt-measure}.

Cette measure est appelle la mesure le \textsc{Lebésgue-Stiltyes}. $F(t)=\pro(X≤t)$.

% subsection rappel_de_th_de_la_measure (end)

\section{Indépendances}

$A$, $B$ deux événements (c'est-à-dire $A, B\in\mathcal{A}$).
\begin{definition}
	On dira que $A$ et $B$ sont \textsc{Indépendants} si $$\pro(A\cap B)=\pro(A)\pro(B).$$
\end{definition}

\begin{definition}
	On appelle $σ$-algèbre engendrée par une variable aléatoire $X$, $σ(X)$ la plus petite $σ$-algèbre pour rapport à la quelle $X$ est mesurable.	
\end{definition}

\begin{proposition}
	$σ(X)=\{X\dmo (B)\ |\ B\in \mathcal{B}\}$
\end{proposition}

\begin{definition}
	Deux v.a. $X$ et $Y$ définies sur le même espace $(Ω,\mathcal A,\pro)$ sont l'indépendantes si $σ(X)$ et $σ(Y)$ sont indépendantes.
\end{definition}
	
On appelle $σ$-algèbre engendrée par une variable aléatoire $X$, $σ(X)$ la plus petite $σ$-algèbre pour rapport à la quelle $X$ est mesurable.

Si $X$ et $Y$ sont indépendantes si
$$P_{XY}(X\in A, Y\in B)=P_X(A) P_Y(B)$$

Produit direct de deux mesure? Considéré $S=S_1\times S_2$.
Di ou construit l'espace mesurable $(S_1\times S_2,\ \mathcal{A}_1\times\mathcal{A}_2)$. Il existe une seule mesure $\bar{\mu}$ telle que:
$$\bar{\mu}(A_1\times A_2)=\mu_1(A_1)•\mu_2{A_2}$$

Cette mesure $\bar{\mu}$ est le produit direct de $\mu_1$ et $\mu_1$, dénote $\bar{\mu}=\mu_1\times\mu_1$.

\begin{theorem}
	Deux variables aléatoire $X$ et $Y$ sont indépendantes ssi la loi conjointe coïncide avec le produit direct des lois marginales.
C'est-â-dire:
$$P_{XY}=P_X\times P_Y$$ % put in box.
\end{theorem}

Ex
$$\int\limits_{\mathbb{R}} f(t,u) \dif P_{XY}(t, u)\mu$$

On a besoin d'une autre quantité; fonction de répartition de deux variables.

\begin{definition}
	Si $X$ et $Y$ sont 2 v.a. ou définit
	$$F_{xy}(u,\ v)=\pro (X\leq u, Y\leq v)$$
\end{definition}

\begin{proposition}
	Si ou connait la fonction de répartition du couple $(X,Y)$ on peut calculer les fonctions de répartition marginales
	$$F_X(u)=\lim\limits_{v\rightarrow +\infty}F_{XY}(u, v)$$
	$$F_Y(v)=\lim\limits_{u\rightarrow +\infty}F_{XY}(u, v)$$
\end{proposition}
\begin{proof}
	$F_X(u)=\pro (X\leq u)=P_X((-\infty, u])$. Utilise $\mathbb{R}=\cup^\infty_{k=1}(-\infty, k]$ $(-\infty, k)$ est croissant. $\pro (X\leq u)=\pro (X\leq u, Y\in\mathbb{R})=\pro (X\leq u, Y\in\cup_{k=1}^\infty(-\infty, k])$. $F_X(u)=\pro (X\leq u)=P_X((-\infty, u])$.
\end{proof}

\begin{proposition}
	Si $X$ est $Y$ sont indépendant v.a. donc $F_{XY}(u, v)=F_x(U) F_Y(V)$
\end{proposition}
\begin{proof}
	...
\end{proof}

\begin{proposition}	
	Si on a: $F_{XY}=(u, v)=F_X(u)F_Y(v)$ cest-a que $X$ et $Y$ sont indépendante? Oui.
\end{proposition}
\begin{proof}
	$$P_{XY}(X\leq u, Y\leq v)=P_X(X\leq u)P_Y(Y\leq u)$$
	la borelien de la forme $\{(-\infty, u], |u|<\infty\}$ vérifier le propriété de l'intersection firme.
\end{proof}

\begin{definition}
	La mesure de lebegue dans $\mathbb{R}^2$ est la mesure produit direct des mesure des lebesgue dans $\mathbb{R}$.
\end{definition}

Convention $\int f\dif \lambda(x)=\int f \dif x$.

\begin{definition}
	Un couple de v.a $(X, Y)$ a une loi conjointe $P_{XY}$ a density si pour toute borelie $B\in\mathcal{B}^{(2)}$ ($\sigma$-algèbre produite), on a
	$$P_{XY}(B)=\iint\limits_{B}f_{XY}(u, v)\dif\lambda(u)\dif\lambda(v)$$. En particulier s ou a $g(u,v)\in L^1(P_{XY})$ on a:
	$$\iint\limits_{\mathbb{R}^2}g(u, v)\dif P_{XY}(u, v)=\iint g(u,v) f_{XY}(u, v)\dif \lambda(u) \dif \lambda(v)$$
\end{definition}

Questions
\begin{enumerate}
	\item Donner les proprettes de $f_{XY}$ quand $X$ et $Y$ sont indépendants.
	\item Si on connait $F_{XY}(u, v)$ est-ce qu'on peut calculer les marginales $f_X(u),\ f_Y{v}$?
\end{enumerate}

\begin{proposition}[générale]
	Si on connait $f_{XY}(u, v)$ on a:
	$\begin{array}{rrl}f_X(u) & = & \int_\mathbb{R}F_{XY}(u,v)\dif v\\ f_X(v) & = & \int_\mathbb{R}F_{XY}(u,v)\dif u\end{array}$
\end{proposition}
\begin{proof}
	$F_X(t)=\lim\limits_{r\rightarrow\infty}F_{XY}(t, r)|=|$\\
	$$F_{XY}(t,r)=\pro (X\leq t, Y\leq r)=P_{XY}((-\infty, t]\times(-\infty, r])=\int\limits_{-\infty}^t\int\limits_{-\infty}^r\dif \lambda(u) \dif \lambda(v)=F_{XY}(t,r)$$
	$|=|\lim\limits_{r\rightarrow\infty}\iint_{-\infty -\infty}^{t\ r} f_{XY}(u, v)\dif \lambda(u) \dif \lambda(v)=\lim\limits_{r\rightarrow\infty}\iint_{-\infty -\infty}^{t\ r} f_{XY}(u, v)\ind (u)\ind (v)\dif \lambda(u) \dif \lambda(v)=$\\
	\text{| Par Fubini ou sont étirer les intégrales: |}\\
	$=\lim\limits_{r\rightarrow\infty}\int_{\mathbb{R}} \dif u \int_{\mathbb{R}} \dif v \ind (u)\ind (v) f_{XY}(u, v)$
	$=\lim\limits_{r\rightarrow\infty}\int_{-\infty}^t \dif u \int_{-\infty}^r \dif v  f_{XY}(u, v) =|\text{B. Levi}|=
	=\int_{\mathbb{R}} \dif u \lim\limits_{r\rightarrow\infty} \int_{\mathbb{R}} \dif v \ind (u)\ind  f_{XY}(u, v)$
	$F_X(t)=\int_{\mathbb{R}} \dif u \int_{\mathbb{R}} \dif v \ind (u)\ind  f_{XY}(u, v)$.
	
	Si $X$ est à densité $F_X(T) = \int\limits_{-\infty}^t f_X(u) \dif u$.
\end{proof}

\underline{Question} (Indépendantes et densités)
\begin{proposition}
	Ou a deux parties.
	\begin{enumerate}
		\item Si 2 v.a. $X$ et $Y$ admettait, des densités $f_X$ et $f_Y$ admettent des densités $f_X$ et $f_y$ et $X$ et $Y$ sont indépendantes, alors le couple (X, Y) admet une loi conjointe a densité et $f_{XY}=f_X f_Y$.
		\item Si le couple $(X, Y)$ admet une densité $f_{XY}$ produit de deux fonctions intégrable $f_1$ et $f_2$ alors $f_1$ et $f_2$ sont les dentistes (à une constant pvit) de $X$ et $Y$ et $X$ et $Y$ sont indépendantes.
	\end{enumerate}
\end{proposition}

\underline{Exercice}
On a un couple de v.a. $(X, Y)$ à valeurs dans $\mathbb{R}^2$ de loi conjointe:
$$P_{(XY)}(B)=\sum_{k=1}^\infty\sum_{l=1}^\infty \frac{1}{2^{k+l}}\delta_{\{k, l\}}(B)$$
Determiner la loi de $Z=\sup \{X, Y\}$.
\begin{enumerate}
	\item{question}. Déterminer $P_X,\ P_Y$ oui $P_X(X=k)$.
Si $X$ et $Y$ sont discrète $\pro (X=k)=\sum\limits_j\pro (X=k,\ Y=j)$. $$P_X(\{x\}=\sum\limits_j P_{XY}(\{k, j\})$$
$$\pro (X=k)=P_X(\{k\})=\sum\limits_{j=1}^\infty \frac{1}{2^{k+j}}$$
$\pro (Z\leq k)=\pro (X\leq k, Y\leq k)=\int \ind _{[0, k]^2}(X, Y)\dif \pro =\iint \ind _{[0, k]^2} \dif P_{XY}(u, v)=\sum\limits_{i,l=1}^\infty \frac{1}{2^{i+l}}\ind _{[1, k]^2}(i, l)=\sum\limits_{i=1}^k\sum\limits_{l=1}^k\frac{1}{2^{i+l}}$
\end{enumerate}





\section{Leçon 4} % (fold)
\label{sec:lesson_4}

Il fallait montrer que si les variables aléatoires $(X_1, X_2)$ ont une densité $f_{X_1X_2}$ produit direct de deux fonctions $f_1$ et $f_2$ , alors à une constante près, $f_1$ et $f_2$  sont le densité de $X_1$ et $X_2$ et ces deux variables sont indépendantes. 

\emph{L'autre partie} (exercice). Si $X_1$ et $X_2$ sont indépendantes de densité respectives $f_{X_1}$ et $f_{X_2}$, alors le vecteur $(X_1,\ X_2)$ a densité: $f_{X_1X_2}=f_{X_1} f_{X_2}$.

\begin{proof}
	\emph{Par hypothèse}, $f_{X_1X_2}(u, v)=f_{X_1}(u) f_{X_2}(v)$. D'autre coté on sait que en général:
	$$f_{X_1}(u)=\int_{\R}f_{X_1X_2}(u,\ v)\dd{\lambda v}$$
	$$f_{X_2}(v)=\int_{\R}f_{X_1X_2}(u,\ v)\dd{\lambda u}$$
	\underline{Objectif}: Montrer que, a une constante près $f_1=f_{X_1}$, $f_2=f_{X_2}$. On observe que:
	$$f_{X_1}(u)=\int_{\R}f_{X_1X_2}(u,\ v)\dd{v}=f_1(u)\int_{\R} f_2(v)\dd{v}$$
	$$f_{X_2}(v)=\int_{\R}f_{X_1X_2}(u,\ v)\dd{u}=f_1(v)\int_\R f_1(u)\dd{u}$$
	On multiple les deux expressions:
	$$f_{X_1}(u)f_{X_2}(v)=f_1(u)f_2(v)\int_\R f_2(v)\dd{v}\int_\R f_1(u)\dd{u}=f_1(u)f_2(v)\iint_{\R\times\R} f_2(v) f_1(u)\dd{u}\dd{v}$$
	Or 
	$$ \int_\R f_2(v)\dd{v}\int_\R f_1(u)\dd{u}=\iint_{\R\times\R} f_2(v) f_1(u)\dd{u}\dd{v}=\iint_{\R\times\R} f_{X_1,X_2}(u,v)\dd{u}\dd{v}=1,$$
	car $f_{X_1,X_2}$ est une densité de probabilité.
	Donc on a montré que $f_1(u)f_2(v)=f_{X_1}(u)f_{X_2}(v)$.
	\begin{remark}
		À des constants prés on pourra identifier $f_{X_1}$ avec $f_1$ et $f_{X_2}$ avec $f_2$. Pour terminer: La loi du couple $(X_1, X_2)$ $P_{X_1X_2}$ on sait que on peut l'écrire. 
	\end{remark}
\end{proof}	
\begin{notations}
	Si on a une mesure $P$ avec densité $f$ on l'écrira comme ça: $P=f\dd{x}$, 	$P(A)=\int_A f\dd{x}$. $\int g\dd{f}=\int g f\dd{x}$.
	
		$$P_{X_1X_2}=f_{X_1X_2}(u, v)\dd{\lambda (u)}\dd{\lambda (v)}=f_1(u)f_2(v)	\dd{\lambda (u)}\dd{\lambda (v)}=f_{X_1}(u)f_{X_2}(v)\dd{\lambda (u)}\dd{\lambda (v)}=P_{X_1}\otimes P_{X_2}$$ (product direct des lois marginales).
		
		$P_{X_1X_2}=P_{X_1}\otimes P_{X_2}$ produit directe de lois marginales. Et on sait que C.N.S\question{what} pour l'independence est que $P_{X_1X_2}=P_{X_1}\otimes P_{X_2}$.
\end{notations}

\begin{proposition}
	Si $X_1$ et $X_2$ sont deux v.a., le trois assertions suivantes sont équivalentes:
	\begin{enumerate}
		\item $X_1$ et $X_2$ sont indépendantes
		\item $\forall$ fonctions $g_1$ et $g_2$ réels et positive on a:
			$$\int g_1\circ X_1 • g_2\circ X_2\dd{\pro}= \int g_1\circ X_1 \dd{\pro}•\int g_2\circ X_2\dd{\pro}$$
		\item Pur tout fonctions réels bornées, $g_1$ et $g_2$ on a:
			$$\int g_1\circ X_1 • g_2\circ X_2\dd{\pro}= \int g_1\circ X_1 \dd{\pro}•\int g_2\circ X_2\dd{\pro}$$
	\end{enumerate}
\end{proposition}

\textbf{Applications.} Supposons que $g_1$ et $g_2$ sont l'identité et que $X_1$ et $X_2$ sont indépendantes:
	$$\mathbb{E}(X_1• X_2)=\mathbb{E}(X_1)•\mathbb{E}(X_2)$$
	$$\int 1\circ X_1 \cdot 1\circ X_2\dd{\pro}= \int 1\circ X_1 \dd{\pro}•\int 1\circ X_2\dd{\pro}.$$
	
\begin{examplebox}
	$X_1$ et $X_2$ indépendant\\$\int X_1^2 \sin X_2\dd{\pro}=\int X_1^2 \dd{\pro} \int \sin X_2\dd{\pro}$.
\end{examplebox}

\begin{remark}
	Si $X_1$ et $X_2$ sont indépendants: $\mathbb{V}(X_1+X_2)=\mathbb{V}(X_1)+\mathbb{V}(X_2)$.
	
	$\mathbb{E}([(X_1=X_2)-\mathbb{e}(X_1+X_2)]^2)$.On développe le carré on découvera des termes de type $\E(X_1X_2)$, on utilisera l'égalité $\mathbb{E}(X_1• X_2)=\mathbb{E}(X_1)•\mathbb{E}(X_2)$ \question{pour définis le fait que 2 variables sont de corrélées}
\end{remark}

\begin{examplebox}
	Sur l'espace probabilité $(\Omega, \mathcal{A}, \pro)$ on considère le couple $(X, Y)$ ave loi conjomte- $P_{XY}$ à densité
	$$f_{XY}(u,v)=\alpha(1-u^2)\ind_{[0,1)}(u)v e^{-3v}\ind_{(0, +\infty)}$$
	\begin{enumerate}
		\item déterminer le valeur de $\alpha$
		\item déterminer la lois marginales.
	\end{enumerate}
\end{examplebox}

\begin{examplebox}
	Sur l'espace $(\Omega,\mathcal{A},\pro)$ on a à nouveau le vecteur aléatoire $(X, Y)$ de loi 
	$$P_{XY}=\alpha(\mu_1+\mu_2+\mu_3)$$
	où $\alpha$ est un paramètre et\\
	$\mu$ est une mesure à densité avec densité:
		$f_1(u, v)=\frac{1}{u^2}e^{-v}\ind_{[1,+\infty)}(u)\ind_{[0, +\infty)}(v)$\\
	$\mu_2$: mesure uniformément distribuée sur $[0,1]\times[0,1]$.\\
	$\mu_3=\delta_{\{1,1\}}+\delta_{\{-1,2\}}$. 
	
	Déterminer $\alpha$ et le lois marginales de $X$ et de $Y$. Est que $X$ et $Y$ sont indépendantes?
\end{examplebox}

\begin{exercise}
		Soit $(X,Y)$ un vecteur aléatoire à valeurs dans $\R^2$.
		\begin{enumerate}[(i)]
			\item suppose que le loi du couple $(X,Y)$ est connue:
			$$d_{XY}(u,v)=\lambda \rho e^{-\lambda u - \rho v}\ind_{\R^2_+}(u,v) \dd{u}\dd{v}$$
			Déterminer la loi de la v.a. $W=\min\{X,Y\}$
			
			\textbf{Deux méthodes} (équivalentes).
			\\1ère méthode: $F_W(t)=\pro(W\leq t)=1-\pro(W>t)=1-\pro(X>t,Y>t)=1-\int\limits_\Omega \ind_{(t,+\infty)}X\cdot \ind_{(t,+\infty)}Y \dd{\pro}=\iint\limits_{\R\times\R} \ind_{(t,+\infty)\times(t,+\infty)}\dd{P_{XY}(u,v)}=\iint\limits_{\R\times\R} \ind_{(t,+\infty)\times(t,+\infty)} \lambda \rho e^{-\lambda u}e^{-\rho v}\dd{u}\dd{v},\ t\geq 0$
$=1-\int_t^\infty \dd{u} \int_t^\infty \dd{v} \lambda \rho e^{-\lambda u} e^{-\rho v}=1-\lambda\int_t^\infty e^{-\lambda}\dd{u} \rho \int_t^\infty e^{-\rho v}\dd{v} = [1-e^{-(\lambda -\rho)t}]\ind_{[0,\infty]}(t)$.

On sait que:
$$F_W(t)=\int_{-\infty}^tf_W(s)\dd{s}.$$
Si on connait $f_X$, ou peut calculer $F_W$?

$F$---distribution function (fonction de repartition).\\
$f$---probability density function (fonction de densité).

$F'_W(t)=(\lambda+\rho)e^{-(\lambda+\rho)t}$
Mais $F'_W=(\lambda+\rho)$ from +, but 0 from -0.		

Il y a 2 cas:\\
(i)	$t\in(-\infty,0)\ F_W(t)=0\Rightarrow f_W(t)=0$\\
(ii) $t\geq 0$ $[1-e^{(\lambda+\rho)t}]=\int_\infty^t f_W(s)\dd{s}$

Est-ce que $X$ et $Y$ sont indépendantes? Yes. $f_X\cdot f_Y$.

Méthode tris générale pour construire des variables aléatoires.

On construit une nouvelle v.a. $g\circ X=Y$. \underline{Question} Si on connaît la loi de $X$, peut on calculer la loi de $Y$?
Ex $X$ a nue loi exp: $f_X(u)=\lambda e^{-\lambda u}$; calcules la loi de $\sqrt[2]{X}=Y$.

Chourinevousm- une fonction test non-négative $h:\R\to\R_+$ et ou eousitere-:
$$\mathbb{E}(h\circ Y)=\int_\Omega h\circ Y\dd{\pro}=\int_\Omega h\circ g\circ X \dd{P}$$
$$\int_\Omega h\circ Y\dd{\pro}=\int_\R h(v)f_Y(v)\dd{v}$$
$$==\int_\R h(g(u))\dd{P_X(u)}=\int_\R h(g(u))f_x(u)\dd{u}$$

On a: $\int_\R h(g(u))f_X(u)\dd(u)=(\mbox{Particular case})=\lambda\int_\R h(\sqrt{u})e^{-\lambda u}\dd{u}$

On pose $\sqrt{u}=v$ $\dd{v}=\frac1{2v}\dd{u}$

$==2\lambda\int_0^\infty h(v)e^{-\lambda v^2}v\dd{v}$.

Loi de $Y=\sqrt{X}$ est: $f_Y(v)=2\lambda v e^{-\lambda v^2} \ind_{[0,+\infty)}(v)$.

\underline{Deux méthode}

$\mathbb{E}(h(W))\overset{\mbox{\tiny si on l'est, comme ca}}{=}=\int_\R h(y)f_W(y)\dd{y}$, $f_W$ la densité de $W$. $h$ -fonction test.

$\mathbb{E}(h(W))=\int_\Omega h\circ W\dd{\pro} = \int_\Omega h\circ \min(X,Y)\dd{\pro}=\iint_{\R\times\R}h(\min(u,v))\lambda e^{-\lambda u}\rho e^{-\rho v}\dd{u}\dd{v}= 
\iint\limits_{\{(u,v), u<v\}}h(u)\lambda e^{-\lambda u}\rho e^{-\rho v}\dd{u}\dd{v}+\iint\limits_{\{(u,v), u>v\}}h(u)\lambda e^{-\lambda u}\rho e^{-\rho v}\dd{u}\dd{v}=
\int_0^{+\infty}h(u)e^{-(\lambda+\rho)u}(\lambda+\rho)\dd{u}$
	
	\item 
	\end{enumerate}


\end{exercise}

On a un vecteur aléatoire $(X,Y)$ a valeurs dans $\R^2$, avec loi:
$$f_{XY}(u,v)= \frac1{4\pi} e^{-\frac u2}\ind_{\{u\geq 0\}} \ind_{[0,2\pi]}$$
Déterminer loi du vecteur aléatoire $(\sqrt X \cos Y, \sqrt X \sin Y)$.

$\omega\rightarrow(X(\omega),Y(\omega))$

$$g: \left\{ \begin{array}{rcl}u&=&g_1(x,y)\\v&=&g_2(x,y)\end{array} \right.$$
$g=(g_1,g_2)$

\begin{align*}
	v(\omega)=g_1(X(\omega), Y(\omega))\\
	u(\omega)=g_2(X(\omega), Y(\omega))
\end{align*}
 U vecteur: $(g_1\circ (X,Y), g_2\circ (X,Y))$.
 
 Test fonction $h$, $h:\R^2\rightarrow \R$.
\begin{multline}
\mathbb{E}(h\circ (X,Y))=\int h\circ (X,Y)\dd{\pro}=\iint h(g(u,v))f_{XY}(u,v)\dd{u} \dd{v}\\=\iint h(g_1(u,v),g_2(u,v))f_{XY}(u,v)\dd{u} \dd{v} \overset{?}{=} \int h(\alpha,\beta)f(\alpha, \beta)\dd{\alpha}\dd{\beta}
\end{multline}

$(X,Y)$ 2 v.a. et exulte ou avait une fonction vectorielle $g=(g_1,g_2)$ $g:\R^2\rightarrow\R^2$ et on définit 2 nouvelle v.a. $U$ et $V$ de cette façon:
$U=g_1\circ (X,Y)$, $V=g_2\circ (X,Y)$ et
$$\left\{\begin{array}{ccc}U(\omega)&=g_1(X(\omega),Y(\omega))\\V(\omega)&=g_2(X(\omega),Y(\omega))\end{array}\right.,\ \omega\in\Omega\text{ (espace des éléments)}$$

Cas particulier (Exercice)
$(X,Y)$ une couple de v.a. de loi  cougointe-
$$f_{XY}=\frac1{4\pi}e^{-\frac u2}\ind_{u\geq 0}(u)\ind_{[0,2\pi]}(v)\dd{u}\dd{v}$$\emph{
Question}. Trouvons la loi du couple:
$$(\sqrt{X}\cos Y, \sqrt X\sin Y)=(U,V)$$
Soit $h:\R^2\rightarrow \R$ une fonction \texttt{test} non-négative.
$\E(h\circ g(X,Y))=\int_\Omega h\circ g(X,Y)\dd{\pro}=\iint\limits_{\R^2}h(g_1(x,y), g_2(x,y))f_{XY}(x,y)\dd{x} \dd{y}\overset{?}{=}$ $\iint\limits_{\R^2}h(u,v)f_{UV}(u,v)\dd{u} \dd{v}$

$$\left\{\begin{array}{ccc}u=\sqrt x \cos y\\ v=\sqrt x \sin y\end{array}\right.$$

$$g: \left\{ \begin{array}{ccc}u&=&g_1(x,y)\\v&=&g_2(x,y)\end{array} \right.$$

$g=(g_1,g_2)$. Pour pouvoir effectuer un changement de variable, il faut que $g$ soit un \underline{difféomorphisme} entre 2 ouverts.
% img omega 

$g:\mathcal O_1\rightarrow \mathcal O_2$ difféomorphisme entre deux ouverts. Condition équivalents pour avoir un difféomorphisme:
\begin{enumerate}[(i)]
	\item $g$ est injective sur $\mathcal O_1$ à valeurs dans $\mathcal O_2$.
	\item $g$ est de classe $b^{(1)}(\mathcal O_1)$, c'est-à dire les dérivés partielles de $g$ existe et sont continuous.
	\item Le déterminant de $\det(g\dmo)'\neq 0$ sur $\mathcal O_2$
\end{enumerate}

Nous avons défini
$$g: \left\{ \begin{array}{ccc}u&=&g_1(x,y)\\v&=&g_2(x,y)\end{array} \right.\overset{\mbox{il faut inverser}}{\rightarrow}$$
$$g\dmo: \left\{ \begin{array}{ccc}x&=&\Phi_1(u,v)\\y&=&\Phi_2(u,v)\end{array} \right.$$
$\Phi=(\Phi_1,\Phi_2)$

On construit la invariante Jacobienne (dérives) de $g\dmo=\Phi$:

$$J_\Phi(u,v)=\mqty(\pdv{\Phi_1}{u}&\pdv{\Phi_1}{v}\\\pdv{\Phi_2}{u}&\pdv{\Phi_2}{v})$$
$$|\det J_\Phi (u,v)|$$

$$=\iint\limits_{g(O_1)=O_2}h(u,v)f_{XY}(\Phi_1(u,v),\Phi_2(u,v))|\det J_\Phi (u,v)|\dd{u}\dd{v}$$

Trouve le nouvelle domaine d'intégration.
$\iint\limits_{O_1}h(g_1(x,y), g_2(x,y))f_{XY}(x,y)\dd{x} \dd{y}=\iint\limits_{g(O_1)=O_2}h(u,v)f_{XY}(\Phi_1(u,v),\Phi_2(u,v))|\det J_\Phi (u,v)|\dd{u}\dd{v}$

La densité de $(U,V)$ est donc:

\begin{empheq}[box=\mymath]{equation*}
	f_{UV}(u,v)=f_{XY}(\Phi_1(u,v),\Phi_2(u,v))\cdot |\det J_\Phi(u,v)|\cdot \ind_{g(\mathcal O_1)}(u,v)
\end{empheq}

Continuer avec l'exercice.:

{\small

$\iint_{0\ 0}^{\infty\ 2\pi} h(\sqrt x \cos y,\sqrt x \sin y)\frac 1{4\pi}e^{-\frac x2}\dd{x}\dd{y}=$

$$g:\ \left\{\begin{array}{ccc}u=\sqrt x \cos y\\ v=\sqrt x \sin y\end{array}\right.$$

$\Rightarrow \Phi: \left\{\begin{array}{rcl}x&=&u^2+v^2 \\ y&=&\arctan(\frac vu) \end{array}\right.$


$=\iint\limits_{\R\times\R}\dd{u}\dd{v} h(u,v) \frac 1{4\pi}e^{\frac{-(u^2+v^2)}2}\cdot |\det J_\Phi (u,v)|$

$$J_\Phi(u,v)=\left(\begin{array}{cc}2u&2v\\ \frac{-v}{u^2+v^2}&\frac u{u^2+v^2}\end{array}\right)$$
$\det J_\Phi (u,v)=\frac{2u^2}{u^2+v^2}+\frac{2v^2}{u^2+v^2}=2$

Question:
\begin{itemize}
	\item $U$ et $V$ sont indépendant? Oui car $f=f_1•f_2$
	\item Lui de $U$ et $V$:
	$\frac 1{\sqrt 2\pi}e^{\frac{-(u^2+v^2)}2}$ et $\frac 1{\sqrt 2\pi}e^{\frac{-(u^2+v^2)}2}$
\end{itemize}

Exercice
Soit $(X,Y)$ une vecteur aléatoire de loi $f_{XY}(x,y)=\frac 1{2\pi}e^{-\frac{x^2+y^2}2}$. Calculer la loi de $(X+Y,Y)$.
Objectif calcul la lui de la somme.

$$\iint_{\R^2}h(x+y, y)\frac 1{2\pi}e^{-\frac{x^2+y^2}2}\dd x\dd y$$
$$g: \left\{ \begin{array}{ccccc}u&=&g_1(x,y)&=&x+y\\v&=&g_2(x,y)&=&y\end{array} \right.$$
$$\Phi: \left\{ \begin{array}{ccc}x&=&u-v\\y&=&v\end{array} \right.$$
$$\abs{\det J_\Phi (u,v)}=1$$
$$=\iint_{\R\times\R}h(u,v)\frac{1}{2\pi}e^{-\frac{(u-v)^2+v^2}{2}}\dd u\dd v$$
Densité de $(X+Y,Y)$ est $\frac{1}{2\pi}e^{-\frac{u^2-2uv+2v^2}{2}}$.
Densité de $(X+Y)$:
$$f_U(u)=\int_{-\infty}^\infty f_{UV}(u,v)\dd v=\frac 1{2\pi}\int_{-\infty}^\infty e^{-\frac 12(u^2-2uv+2v^2)}\dd v$$
---produit de convolution.
}

\begin{theorem}
	Si $X$ et $Y$ sont indépendant de loi marginal $f_X$ et $f_Y$ alors la v.a. $X+Y$ est à densité est $f_{X+Y}(u)=\int_\R f_Y(u)\cdot f_X(u-v)\dd v\overset{\mbox{\tiny def}}{=}f_Y\ast f_X$---produit de convolution.
\end{theorem}



Soit (U,V) un couple de variables aléatoires de densité conjointe:
$$f_{UV}(u,v)=\left\{\begin{array}{l}γ(2u^2v+1), (v,v)\in D\\ 0 \text{, ailleurs}\end{array}\right.$$
où $D=\{(u,v)\in \R^2\ |\ |u|<1, |u-1|<1\}$.

\begin{exercise}
	\textbf{Questions.}
	\begin{enumerate}
		\item déterminer la valeur de $γ$
		\item déterminer si $(U,V)$ sont \emph{indépendantes}
		\item déterminer si $(U,V)$ sont \emph{corrélées} 
		\item déterminer la loi du couple $(A,B)$: où $A=U•V,\ B=V$
	\end{enumerate}
	
	$h$: fonction test:
	$\int h(A,B)\dd{\pro}=\int h(U•V,V)\dd{\pro}=\iint_D h(uv,v)f_{UV}(u,v)\dd{u}\dd{v}=\iint_{D'}h(a,b)f(...)\dd a\dd b$
	$$J_{g\dmo}(a,b)=\mqty(\frac 1b&-\frac a{b^2}\\0&1)$$
	$$g:\left\{ \begin{array}{l}a=uv\\b=v\end{array}\right.\ g\dmo:\left\{ \begin{array}{l}u=\frac ab\\v=b\end{array}\right.$$
	$|\det J_{g\dmo}|=\frac 1b$
	$=\frac 3{20}\iint_{D'=g(D)}h(a,b)(2\frac {a^2}b+1)\frac 1b\dd a\dd b$
	$\Rightarrow$ $f_{AB}(a,b)=\frac 3{20}(2\frac {a^2}b+1)\frac 1b\ind_{D'}(a,b)$
	Draw $D'=g(D)=\{(a,b), b\in [0,2]\ -b<a<b\}$
	Is it difféomorphisme?
	Yes, car ...
\end{exercise}

\begin{definition}
	Si $X$ et $Y$ sont $2$ v.a. on définit la \textsc{Covariance} entre $X$ et $Y$ comme
	$\cov(X,Y)\overset{\text{def}}{=}\E\left[(X-\E(X))(Y-\E(Y))\right]=\E(XY)-\E(X)\E(Y)$.
\end{definition}

\begin{exercise}
	Soit $X$ une v.a. de loi $N(0,1)$ et $Y$ une v.a. discrète de loi $\frac 12\{δ_{(-1)}+δ_{1}\}$ indépendante de $X$.
	\begin{itemize}
		\item Montrer que la loi de $Z=X•Y$ est $N(0,1)$ 
		\item Montrer que $(X,Z)$ ne sont pas corrélées 
		\item Calculer $\E(X^2Z^2)$
		\item Déterminer si $(X,Z)$ sont indépendantes
	\end{itemize}
	
	Loi de $Z$.
	
	$F(t)=\pro(Z≤t)=\int_\R \ind_{\{x•y≤t\}}(x•y)\dd{P_{XY}(x,y)}\overset{indépendantes}{=}$ $\int\int \ind_{\{x•y≤t\}}(x•y)\dd{P_Y}\dd{P_X}$ $=\int \dd{P_X}(\frac 12 \ind_{-x≤t}(-x)+\frac 12 \ind_{+x≤t}(x))=\frac 1{\sqrt 2\pi}\int \dd{x}e^{-\frac{x^2}2}(\frac 12 \ind_{-x≤t}(-x)+\frac 12 \ind_{+x≤t}(x))$ $+\frac 1{\sqrt 2\pi} \frac 12 \int_{-∞}^t\dd x e^{-x^2/2}+\frac 1{\sqrt 2\pi} \frac 12 \int_{-t}^∞\dd x e^{-x^2/2}=\frac 12 \int_∞^t e^{-\frac{x^2}2}\dd{x}\simeq N(0,1)$
\end{exercise}

\begin{exercise}
	Soient $X$ et $Y$ deux va indépendantes dont $X$ est un variable de Bernoulli $B(1/2)$ et $Y$ suivra deux lois 
	i $Y$ est une variable normale $N(0,1)$ 
	ii $Y$ a fonction de répartition $F_y(t)=t t\in [0,1]$. Dans les deux cas calculer la fonction de répartition $Z=X•Y$.
\end{exercise}


\section{Fonction génératrice} % (fold)
\label{sec:donction_generatrice}
$X$ set une v.a. discrète a valeurs dans $\N\cup\{0\}$ 

Ex
Bernoulli $(0,1)$ $B(p)$ $P_X=(1-p)δ_0+pδ_1$
Binomiale $B(n,l)$ valeurs $1,...,N$ $\pro(x=k)=C_n^kp^k(1-p)^{N-k}$
Géométrique
Valeurs de $G=\{0,1,2,...\}$ $\pro(G=k)(1-p)^{k-1}p$
Poisson valeurs $0,1,2, ...$  $\pro (p=k)=\frac {e^{-λ}λ^k}{k!}$

Résultat très intéressant
Binomiale $N$ très grand $p$ très petite $N•p=O(1)$
$\pro (X=k)=C_n^kp^k(1-p)^{N-k}~G(N•p)$

\begin{definition}
\textsc{Fonction Génératrice} de $X$:
$$g_x(s)=∑_{i=0}^{+∞} p_is^i $$
(série entière, "power" séries), où la loi de $X$ est $P_X=∑_{i=0}^∞p_iδ_{\{i\}}$.	
\end{definition}

\begin{proposition}
	Si on connait $g_X(s)$ on connaît la loi, c-a-d les $\{p_i\}_{i=0}^∞$. D'abord la série converge pour $s\in[-1,1]$ et uniformément pour $s\in ]-1,1[$.
\end{proposition}

\begin{proof}
	La série peut être dérivée terme-à-terme pour $s\in (-1,1)$. 
	\begin{align*}
		g'_X(s)=∑^∞_{i=1}p_iis^{t-1}\\ 
		g''_X(s)=∑^∞_{i=2}p_ii(i-1)s^{t-1}\\
		\cdots
		g^{(k)}_X(s)=∑^∞_{i=k}p_ii(i-1)...(i-k+1)s^{i-k}\\	
	\end{align*}
	On calcul $g^{(k)}(0)$.
	$$p_k=\frac{g^{(k)}(0)}{k!}$$
\end{proof}

\textbf{Attention.} $g_X(s)-∑^∞_{i=0}p_is^i$.
On dérive $g'_X(s)=∑^∞_{i=1}p_iis^{i-1}$.
Supposons qu'on puisse étendre la dérivée dans $s=1$
$g'_X(1)=∑^∞p_ii=\E(X)$

\textbf{Application.} 

\begin{itemize}
	\item $X\sim B(N,p)$ v.a.\\
	$\E(x)=∑^N_{k=0}kC_N^kp^k(1-p)^{N-k}=?Np$
	On utilise la fonction génératrice:
	$g_X(s)=∑^N_{k=0}C_N^kp^k(1-p)^{N-k}s^k= ∑C_n^k(ps)^k(1-p)^{N-k}=[ps+(1-p)]^N$
	$g'_X(s)|_{s=1}=N[p+(1-p)]^{N-1}•p|_{s=1}=Np$
	\item $X$ v.a. Poisson
	$\E(P)=λ$
	$\E(P)=∑_{k=0}^∞ k\frac{λ^ke^{-λ}}{k!}$
	$g_P=∑_{k=0}^∞...=e^-λ e^λs $
\end{itemize}

\begin{lemme}(Abel)
	\begin{enumerate}
		\item Si la série $∑^∞_{i=0}α_i=α$ alors $\lim_{s\to 1^-}∑_{i=0}^∞ α_is^i=α$
		\item Si les $α_i$ sont positifs et si $\lim_{s\to 1^-}∑_{i=0}^∞ α_is^i=α<+∞$, alors $∑_{i=0}^∞α_i=α.$
	\end{enumerate}
\end{lemme}
\begin{proof}
$g_X(s)=∑_{i=0}^∞p_is^i$
(1) Supposons que $\E(X)<+∞$ $\Leftrightarrow$ $∑_{i=0}^∞ ip_i=\E(X)$
donc on est dans la partie 1 du Lemme
Donc 
$$\lim_{s\to1^-}\underbrace{∑_{i=0}^∞ip_i s^i}_{g'_X(s)}=\E(X)$$	
\end{proof}

\begin{proposition}
	On a $\E(X(X-1)...(X-r+1))=\lim_{s\to 1^-}g_X^{(r)}(s):=g_X^{(r)}(1)$.
	Cas particulier $\V(X)=g_X''(1)+g'(1)-[g'_X(1)]^2$
\end{proposition}

\subsection{Fonction génératrice des moments} % (fold)
\label{sub:subsection_name}

$Χ$ une v.a. quelconque $u\in \R$; $G_X(u)=\E(e^{uX}) =\int_Ωe^{uX}\dd{\pro}=\int_\R e^{uX}\dd{P_X{x}}$

Fonction génératrice pour de v.a. à valeurs dans $\N\cup\{0\}$. Si $X$ est une telle v.q. sa loi $P_X=∑_ip_iδ_{\{i\}}$ avec fonction génératrice
$$G_X(s)=∑_{i=0}^{+∞}p_is^i,$$ $|s|≤1$ avec convergence uniforme pour $|s|<1$.

\begin{exercise}
	Soit $X$ et $Y$ deux v.a. à valeurs dans $\N\cup\{0\}$ et indépendantes. Calculer la fonction génératrice de la v.a. $Z=X+Y$.
\end{exercise}
\begin{definition}
	Si $X$ est une v.a. quelconque, on définit, pour $v\in\R$ $g_X(u)=\E(e^{uX})$ la fonction génératrice des moments.
\end{definition}
\begin{remark}
	Si $X$ est à valeurs dans $\N\cup\{0\}$ et si on pose $e^u=s$, on retrouve la fonction génératrice.
\end{remark}

Propriétés de $g_X(u)=\E(e^{uX})$
\begin{enumerate}
	\item $g_x(u)$ est toujours défini pour $u=0$
	\item Si $X$ est borne alors $g_X$ est bien défini et continue pour $u\in\R$ (borne est limite)
		$$g_X(u)=\int_Ω e^{uX(ω)}\dd{\pro(ω)}$$
		$\lim_{u\to u_0}g_X(u)=\lim_{u\to u_0}∫_Ωe^{uX(ω)}\dd{\pro(ω)}$
		\begin{rappel}
			$\lim_{ρ\to \bar ρ}∫f(x,ρ)\dd{μ(x)}\oeq$\\
			Si:\\
			(i) $\lim_{ρ\to \bar ρ}f(x,ρ)$ existe\\
			(ii) $|f(x,ρ)|≤h(x)$ $h≥ 0$ indépendante de $ρ$ et $∫h(x)<∞$\\
			alors $\oeq∫\lim_{ρ\to \bar ρ}f(x,ρ)\dd{μ(x)}$.
		\end{rappel}

		
	\item Si $a,b\in\R$ ,$g_{aX+b}(u) = e^{ub}g_X(au)$
	\item $g_X(-u)$ est la fonction génératrice des moments de la v.a. $Y=-X$
	\item $g_X$ est convexe.
	\item Si $X$ et $Y$ sont indépendantes et chacune admet une fonction génératrice, alors
		$g_{X+Y}(u)=g_X(u)g_Y(u)$
\end{enumerate}

\begin{exercise}
	Soit $Y=N(0,1)$ montrons que $X=e^Y$ a une fonction génératrice des moments qui ne pas définie pour $u>0$.
\end{exercise}

\begin{remark}[important]
	La fonction génératrice des moments est \texttt{importante et utile} si elle existe pour $u$ dans un voisinage de 0.
\end{remark}

\begin{theorem}
	Soit $X$ une v.a. et $g_X(u)$ sa fonction génératrice des moments en définie pour $-u_0<u<+u_0$ (intervalle ouvert), alors:
	\begin{enumerate}
		\item $\E(|X|^k)<∞$, $\forall k≥0$
		\item $\forall u\in]-u_0,u_0[$: $$g(u)=1+\E(X)u+\frac{u^2}{2!}\E(X^2)+...+\frac{u^n}{n!}\E(X^n)+...$$ (série convergente)
		\item $\forall k≥1 g_X^{(k)}(u=0)=\E(X^k)$---moment d'ordre $k$ de $X$.
	\end{enumerate}
\end{theorem}

\begin{exercise}
	$X=N(0,1)$ $f_X(x)=\frac 1{\sqrt{2π}}e^{-\frac{x^2}2}$. $\E(X)=0$ $\V(X)=1=\frac 1{\sqrt{2π}}∫_{-∞}^{∞}x^2e^{-\frac{x^2}2}\dd{x}$.
	$g_X(u)=\E(e^{uX})=\frac 1{\sqrt{2π}}∫_{-∞}^{∞}e^{ux}e^{-\frac{x^2}2}\dd{x}=\frac 1{\sqrt{2π}}e^{\frac 12 u^2}∫_{-∞}^∞e^{-\frac 12 y^2}\dd{y}=e^{\frac 12 u^2}$.
	$g_X''(u)=e^{\frac 12 u^2}+u^2e^{\frac 12 u^2}|_{u=0}=1=\V(X)$.
\end{exercise}
\begin{exercise}
	Trouver $g_X(u)$ pour $X=\mathcal{E}(λ)$, où $\mathcal{E}$ est la v.a. exponentielle de paramètre $λ$.
\end{exercise}
\begin{exercise}
	Loi de Cauchy: $f_X(x)= \frac{1}{πγ[1+(\frac{x-x_0}{γ})^2]}$
	Pour $x_0=0$, $γ=1$ $f_X(x)=\frac 1{π[1+x^2]}$. $\E(X)=∞$.
	$g_X(u)=∫_{-∞}^{∞}\frac{e^{ux}}{π[1+x^2]}\dd{x}=\underbrace{∫_{-∞}^0\frac{e^{ux}}{π[1+x^2]}\dd{x}}_{<+∞}+\underbrace{∫_0^{∞}\frac{e^{ux}}{π[1+x^2]}\dd{x}}_{+∞}$.\\
	$u=0$ bon, toujours\\
	$u<0$
\end{exercise}
\begin{proof}
	Le preuve sera faite pour des v.a. à densité. On appelle $f_X(x)$ la densité. Donc: $\E(|X^k|)=∫_\R|x^k|f_X(x)\dd{x}$ On sait que: $e^{s|x|}=∑_{k=0}^{∞}\frac{s^k|x|^k}{k!}$ ($\Rightarrow$ $|x|^k≤\frac{e^{s|x|}k!}{s^k})$. On sait par hypothèse que $\E(e^{uX})=∫_{-∞}^∞e^{ux}f_X(x)\dd{x}<∞$ pour tout $u$: $-u_0<u<u_0$. 
	$\E(|X^k|)=∫_\R|x^k|f_X(x)\dd{x}≤\frac{k!}{s^k}∫_\R e^{-s|x|}f_X(x)\dd{x}$
	Si on choisit $-u_0<s<u_0$ l'intégrale $∫_{-∞}^{∞}\underbrace{e^{sx}f_X(x)}_{g_X(s)}\dd{x}<∞$ car et de la même manière $∫_{-∞}^∞e^{sx}f_X(x)\dd{x}=g_{-x}(s)<+∞$
	$≤\frac{k!}{s^k}[∫_{-∞}^∞e^{-sx}f_X(x)\dd{x}+∫_{-∞}^∞e^{sx}f_X(x)\dd{x}]$ (voir une des proposition affichée)
	
	$\E(g(x))=∫g(x)f_X(x)\dd{x}$, $\E(-x)=∫-xf_X(x)\dd{x}$.
\end{proof}
\begin{proof}[Preuve de (2)]
	On sait que $-u_0<u<u'_0<u_0$ $g_X(u)=∫_\R e^{ux}f_X(x)\dd{x}=∫_\R ∑_{k=0}{∞} \frac{(ux)^k}{k!} f_X(x)\dd{x}\overset?= (à justifier!)=∑_{k=0}^{∞}\frac{u^k}{k!}∫_\R x^kf_X(x)\dd{x}=∑_{k=0}^∞\frac{u^k}{k!}\E(x^k)$.
	
	\emph{Question} interchangeable la limite avec l'intégrale.
	$∫_\R\lim_{n\to ∞}∑_{k=0}^n \frac{(ux)^k}{k!}f_X(x)\dd{x}$
	(1)cette limite doit exister mais ça c'est vrai: $f_X(x)e^{ux}$.
	(2)$|∑_{k=0}^n\frac{(ux)^k}{k!}f_X(x)|≤f_X(x)∑_{k=0}{n}\frac{|u|^k|x|^k}{k!}≤f_X(x)∑_{k=0}{∞}\frac{u_0^k|x|^k}{k!}≤f_X(x)e^{u_0'(x)}$
	
	Nous reste à montrer que $∫_\R e^{u_0'|x|}f_X(x)<∞$.
\end{proof}

\begin{itemize}
	\item Fonction génératrices
	\item Fonction génératrices des moments.
	\item Fonction caractéristique (on pourra démontrer Hiremeieme centrale limite)
	\item (caractérisation la convergence en distribution en loi)
\end{itemize}

\begin{definition}
	Soit $X$ une v.a. (vectorielle); on pose:
	$$φ_X(t)=\E(e^{itX}),\ i=\sqrt{-1}$$
	On appelle $φ_X(t)$ \textsc{la Fonction Caractéristique} de $X$.
\end{definition}
\begin{remark}
	Si $X$ est vectorielle $tX=∑_{i=1}^{d}t_ix_i$, $t=(t_1,...,t_d)$. Pour un moment on considère $X:Ω\rightarrow \R$ (salaire, pas vectorielle).
\end{remark}
\begin{remark}
	Si $X:Ω\rightarrow \R$ a densité $f_X(x)$ alors $φ_X(t)=∫_\R e^{itx}f_X(x)\dd{x}$ transformée de Fourrier de $f_X$. 
\end{remark}
\begin{remark}
	En général si $X$ a loi $P_X$, on a $φ_X(t)=∫_\R e^{itx}\dd{P_X(x)}$. On avait dit que la loi $P_X$ est la mesure de Lebegue-Stilties engendres par la fonction de répartition $F_X$ de $X$. $P_X((a,b])=F_X(b)-F(a)$.
\end{remark}
\begin{proposition}
	\leavevmode
	\begin{enumerate}
		\item $φ_X(t)$ est définie et continue pour $t\in\R$ est aussi bornée.
		\item $φ_{aX+b}(t)=e^{ibt}φ_X(at)$
		\item (Importante!) Soit $X_1,X_2, ..., X_n$ une suite de v.a. toutes définies sur la même espace $(Ω,\mathcal{A},\pro)$ et indépendantes.
	\end{enumerate}
\end{proposition}

On définit $S_n=X_1+X_2+...+X_n:Ω\rightarrow \R$.
$φ_{S_n}(t)=\E(e^{itS_n})=\E(e^{it(X_1+...+X_n)})=\E(e^{itX_1}...e^{itX_n})=\prod_{j=1}\E(e^{itX_j})$
\begin{example}
	$X=N(0,1)$. On a: $φ_X(t)=e^{-\frac{t^2}2}$ (très important pour le Th. central limite)
	$e^{itx}=\cos tx+i\sin tx$
	$φ_X(t)=\frac 1{\sqrt{2π}}∫_{-∞}^{∞} e^{itx}e^{-\frac{x^2}2}\dd{x}=\frac 1{\sqrt{2π}}∫_{-∞}^{∞} \cos{tx} e^{-\frac{x^2}2}\dd{x}+ \underbrace{\frac i{\sqrt{2π}}∫_{-∞}^{∞} \sin{tx} e^{-\frac{x^2}2}\dd{x}}_0 =\frac 1{\sqrt{2π}}∫_{-∞}^{∞}\cos{tx} e^{-\frac{x^2}2}\dd{x}$
	On sait que:
	$φ_Χ(τ)=\frac 1{\sqrt{2π}}∫_{-∞}^{∞}\cos{tx} e^{-\frac{x^2}2}\dd{x}$. On dérive par rapport à t.
	$φ'_Χ(τ)=\frac 1{\sqrt{2π}}d/dt∫_{-∞}^{∞}\cos{tx} e^{-\frac{x^2}2}\dd{x}==$
	$\exists d/dt \cos{tx} e^{-\frac{x^2}2}=-\sin{tx} e^{-\frac{x^2}2} x$
	$==\frac t{\sqrt{2π}}φ_X(t)$
	Donc $\frac t{\sqrt{2π}}φ_X(t)=φ'_Χ(τ) φ_Χ(0)=1$:
	$∫_{φ(0)}\frac 1φ\dd{φ}=∫_0^t\frac t(\sqrt{2π})\dd{t}$
	$φ(t)=e^{\frac 1{\sqrt{2π}}\frac{t^2}2}$
\end{example}

$(a+b)^N=∑_{k=0}^{N}C_n^ka^kb^{N-k}$
\begin{enumerate}
	\item binomiale $B(N,p)$ $q=1-p$
	$φ_X(t)=(q+pe^{it})^N$
	$φ_X(t)=∑_{k=0}^{N}e^{itk}C_n^kp^kq^{N-k}$
	$φ_X(t)=∑_{k=0}^{N}C_n^k(e^{it}p)^kq^{N-k}$
	\item Exponentielle $\mathcal{E}(λ)$: $f_X(x)=λe^{-λx}\ind_{[0,+∞)}(x)$
	$φ_X(t)=\frac λ{λ-it}$
	\item Poisson (discrète) loi $\pro(X=k)=\frac{λ^ke^{-λ}}{k!}$
	Exercice: Calculer loi fonction caractéristique pour Exponentielle et Poisson.
	
\end{enumerate}
\begin{theorem}
	Soit $X:Ω\rightarrow \R et φ_X(t)$ sa fonction caractéristique. On a:
	\begin{enumerate}
		\item $φ_X$ est uniformément continue sur $\R$
		\item Si $\E(|X|^2)<∞$, $\forall n≥1$ alors $φ^{(r)}_X(t)$ existe pour $r≤n$ et $φ_Χ^{(r)}(t)=∫_\R (ix)^2 e^{itx}\dd{F_X(t)}$ et $\E(X^r)=\frac{φ^{(r)}(0)}{i^r}$
		$Φ_X(t)=∑_{i=0}{n}\frac{(it)^2}{r!}\E(X^2)+\frac{(it)^r}{n!}E_n(t)$ avec $|E_n|≤3\E(|X|^n)\to 0$, $t\to 0$.
		\item Si $Φ_X^{(n)}(0)<+∞$ alors $\E(X^{2n})<∞$
		\item Si $\E(|X|^n)<∞$, $\forall n≥0$ et $\lim_{n\to ∞}\frac{\E(|X|^n)^\frac{1}{r}}{n}=\frac 1{eR}<+∞$ alors 
		$φ(t)=∑_{n=0}^∞ \frac{(it)^n}{n!}\E(X^n)$, $|t|<R$
	\end{enumerate}
\end{theorem}
\begin{rappel}
Critère de Cauchy, $∑_{n=0}^∞a_nX^n$ le rayon de convergence $R$ est donne par $\lim_{n\to ∞}|a_n|^\frac 1n =\frac 1R $.
\end{rappel}
\begin{proof}
	(ii) on sait que pour un certain n
	$\E(|X|^n)<∞$, alors $\forall r≤n$, $\E(|X|^2)<∞$ car $L^n(\pro)\subset L^r(\pro)$ si $\pro$ est une mesure de probababilité. On va montrer la formule pour $r=1$, pour $r1$ la preuve est singulière. On applique la définition de dérivée.
	$\frac{φ(t+h)-φ(t)}h=∫_\R\frac{e^{i(t-h)-e^{it}}{h}}\dd{F(x)}=∫_\R\frac{e^{itx}(e^{ihx}-1)}{h}\dd{F(x)}$.
	Considérons cette partie $\frac{e^{ihx}-1}h=\frac{1+ihx+O(|x|^2)-1}{h}$. On développe $e^{ixh}$ en $X$ autour de $0$: $e^{ihx}=1+ihx+O(|x|^2)$
	$\frac{|e^{ihx}-1|}h=\frac{|e^{ihx}-e^{ih0}|}h=\frac{|e^{ih\eta}hx|}h=|x|$
	
	$\lim_{h\to 0}\frac{φ(t+h)-φ(t)}h\overset?= \lim_{h\to 0}∫\frac{e^{i(t-h)-e^{it}}}{h}\dd{F(x)}$
	
	Peut-on ramener la limite dans l'intégrale?\\
	--- la limite dans l'intégrale doit exister\\
	--- la valeur absolue de (Fonction à l'intérieur de l'intégrale) doit être borne par une fonction sommable et indépendante de h
	
	$φ_X^{(r)}(t)=∫_\R(ix)^2 e^{itx}\dd{F_X(t)}.$
	
	$\lim_{h\to 0} \frac{e^{i(t-h)}-e^{it}}{h} = ix e^{itx}$
	
\end{proof}
\begin{theorem}
	Soit $F$ et $G$ deux fonctions de répartition (on deux lois) avec la même fonction caractéristique
	$$∫e^{itx}\dd{F(x)}=∫e^{itx}\dd{G(x)}, \forall t\in \R$$
	alors $F=G$.
\end{theorem}
On a besoin d'une résultat technique. Toute fonction réelle continue sur l'intervalle $[-n,n]$ avec les mêmes valeurs sur les bords, peut être uniformément approximée par des polynômes trigonométriques.

$f_{ε,η}(x)=∑_{k=1}^{N<∞}a_k\exp(iπx\frac kη),\ a_k\in\R$

$\sup_{-n≤X<n}|f_{ε,η}-f_ε(x)|<δ_n\to 0 n\to∞$

\begin{exercise}
	Soient $X$ et $Y$ deux v.a. indépendantes ses lois exponentielles respectivement$ λe^{-λx}$, $μe^{-μx}$. Posons:\\
	$U=\min(X,Y)$\\
	$V=\max(X,Y)$
	$W=V-U$
	\begin{enumerate}
		\item Calculer $\pro(U=X)$
		\item Montrer que $U$ et $W$ sont indépendantes. (Idée estimer d'abord $\pro(U≤u, W>w)$ en décomposant l'élément $(U≤u, W>w)$ sur le système complet d'éléments $\{X≤Y,X>Y\}$.
		\item Calculer la loi de $V$.
	\end{enumerate}
\end{exercise}

\begin{theorem}
	Si $∫e^{itx}\dd{F(x)}=∫e^{itx}\dd{G(x)}$ alors $F(x)=G(x)$.
\end{theorem}
Tout caractéristique égale $\Rightarrow$ Lois égale.
\begin{proof}
	On a besoin de considérer la fonction suivante.
	
	$$f_{ε,n}(x)=∑_{k=1}^N a_k\exp(iπx\frac kn)$$
	
	$\sup_{-n≤x≤n}|f_{ε,n}(x)-f_ε(x)≤δ\to 0\ n\to+∞$
	On fait la preuve (pour la simplicité) avec des densité $f(x),\ g(x)$.
	$\sup_{x\in\R}|f_{ε,n}(x)|=\sup_{x\in(-n,n)}|f_{ε,n}(x)|=\sup_{x\in(-n,n)}|f_{e,ν}-φ_e(χ)|+\sup_{x\in(-n,n)}|f_ε(x)|≤δ_n+1≤2$.
	
	$$∫_\R f_{ε,n}(x) h(x)\dd{x}=∑_{k=1}^N a_k∫ \exp(iπx\frac kn) h(x)\dd{x}=∑_{k=1}^Na_k∫\exp(iπx\frac kn)g(x)\dd{x}=∫_\R f_{ε,n}g(x)\dd{x}$$
	
	$$∫_\R f_{ε,n}(x)h(x)\dd{x}=∫_\R f_{ε,n}(x)g(x)\dd{x}$$
	
	$$|∫_\R f_ε(x)h(x)\dd{x} -∫_\R f_ε(x)g(x)\dd{x}| = |∫_\R f_ε h\dd{x}-∫_\R f_ε g\dd{x} + ∫_\R f_{εn} h\dd{x}-∫_\R f_{εn} h\dd{x} +∫f_{εn}g\dd{x}-∫f_{εn}g\dd{x}|=$$ $|∫_\R(f_ε-f_{εn})h\dd{x}|+|∫(f_ε-f_{εn})g\dd{x}| + |∫f_{εn}g\dd{x}| + |∫f_{εn}h\dd{x}|≤ δ_n + 2F_h(-n)+2(1-F_h(n))+δ_n+2F_g(-n)+2(1-F_g(n)) \to 0$
	$$∫_\R f_ε(x)h(x)\dd{x}=∫_\R f_ε(x)g(x)\dd{x}$$
	
	$$\lim_{ε\to 0}∫_\R fε(x)h(x)\dd{x}=\lim_{ε\to}∫_\R f_ε(x)g(x)\dd{x} \Rightarrow a,b arbitraire ∫_a^bh(x)\dd{x}=∫)a^bg(x)\dd{x}.$$
	
\end{proof}

SSi deux v.a. ont la même fonction caractéristique, elles ont la même loi.

Si $C$ est une v.a.
$φ_X(t)=∫_Ωe^{itX}\dd{\pro}=∫_\R e^{itX}\dd{P_X(x)}\overset{==}{densité}∫_{-∞}^∞e^{itx}f_X(x)\dd{x}$

\underline{Objectif}. Si on connaît $φ_X(t)$, peut-on calculer la loi $P_X(x)$? Ou la densité? Si $φ_X(t)=∫_{-∞}^{∞}f_X(x)\dd{x}$---transformée de Fourier. Comment peut-on calculer $f_X(x)$?

\begin{rappel}
	Si $φ_X\in L^1(\R, Lebesgue)$ $\Rightarrow$ $f_X(x)=\frac 1{\sqrt{2π}}∫_{-∞}^∞e^{-itx}φ_X(t)\dd{t}$.
\end{rappel}
\begin{theorem}[formule d'inversion]
	Soit $φ(t)$ la fonction caractéristique de la fonction de répartition $F(x)$. C'est-à-dire $φ(t)=∫_\R e^{itx}\dd{F(x)}$. On a:
	\begin{itemize}
		\item Pour deux points a<b où F est continue
		$$F(b)-F(a)=\lim_{c\to +∞}\frac{1}{2π}∫_{-c}^c\frac{e^{-ita}-e^{-itb}}{it}φ(t)\dd{t}$$
		Si $∫_{-∞}^∞|φ(t)|\dd t<∞$, et $F$ à une densité $f$ alors
		$f(x)=\frac 1{2π}∫_{-∞}^∞e^{-itx}φ(t)\dd{t}$ (transformée de Fourier inverse)
	\end{itemize}
\end{theorem}
$F(b)-F(a)=∫_a^bf(x)\dd{x}=\frac 1{2π}∫_a^b(∫_{-∞}^∞e^{-itx}φ(t)\dd{t})\dd{x} = -\frac{1}{2π}∫_{-∞}^∞φ(t)(\frac{e^{-itb}-e^{-ita}}{it})\dd{t}$
\begin{proof}  
	Introduisons la quantité $Φ_c=\frac 1{2π}∫_{-c}^c\frac{e^{-ita}-e^-{itb}}{it}φ(t)\dd{t}=\frac{1}{2π}∫_{-c}^c\frac{e^{-ita}-e^-{itb}}{it}[∫_{-∞}^∞e^{itx}\dd{ F(x)}]\dd{t} $
	Supposons de pouvoir interchanger les intégrales
	$= \frac{1}{2π}∫\dd{F(x)}[∫_{-c}^ce^{itx}\frac{e^{-ita}-e^-{itb}}{it}\dd{t}]$, $[]$ - $Φ_c(x)$
	Si $Φ_c(x)$ vérifie $∫|Φ_c(x)|\dd{F(x)}<∞$ on peut interchanger les intégrales.
	$Φ_c(x)=∫_{-c}^ce^{itx}\frac{e^{-ita}-e^-{itb}}{it}\dd{t}$, $Χ$-inside int
	Montrer que $Χ(t)$ est sommable par rapport à $t$. C'est vrai!$ e^{-ita}=1-ita+o(t^2) e^{-itb}=1-itb+o(t^2)$
	Si on fait le calcul explicite on trouve.
	$Φ_c(x)=\frac 1{2π}∫_{-c}^c \frac{\sin t(x-a)-\sin t(x-b)}{t}\dd{t}=\frac 1{2π}∫_{-c(x-a)}^{c(x-a)}\frac{\sin v}{v}\dd{v}-\frac 1{2π}∫_{-c(x-b)}^{c(x-b)}\frac{\sin v}{u}\dd{u}=\frac 1{2π}∫\dd{F(x)}[∫_{-c}^ce^{itx}\frac{e^{-ita}-e^-{itb}}{it}]$
	La fonction
	$f(s,t)=∫_s^t\frac{\sin v}{v}\dd{v}$
	On peut montrer que $g(s,t)\to π$ quand $t\to+∞, s\to-∞.$
	Passons à la limite $c\to+∞$ dans $Φ_c(x)$ ($C>0$) La fonction $Φ_c(x)$ converge vers $Φ(x)$ donnée par:
	$Φ(x)= 0$ si $x\not\in(a,b)$, $1/2$ si $x=a$ ou $x=b$, $1$ si $x\in(a,b)$
	et donc $Φ$ est bornée.
	
	$=\frac 1{2π}∫\dd{F(x)}Φ_c(x)\dd{x}\overbrace{c\to∞}{\to}\frac{1}{2π}∫\dd{F(x)}Φ(x)$.
	$\lim_{c\to ∞}Φ_c=\lim_{c\to ∞}\frac 1{2π}∫_{-c}^c\frac{e^{-ita}-e^-{itb}}{it}φ(t)\dd{t}=\frac 1{2π}∫φ(x)\dd{F(x)}$
	
	C'est quoi $\frac 1{2π}∫φ(x)\dd{F(x)}$ avec $F$ une fonction de répartition?
	
	La mesure de Lebesgue-Stiltjes $\dd{F}$ engendre par $F$ vérifie $\dd{F([a,b])}=F(b)-F(a)$
	
	$\frac{1}{2π}∫_{-∞}^aφ(x)\dd{F(x)}=0$
	$\frac{1}{2π}∫_{\{a\}}φ(x)\dd{F(x)}=0$
	
	$\frac{1}{2π}∫_{\{a\}}φ(x)\dd{F(x)}=\frac{1}{2π}φ(a)[F(a)-F(a-0)]$
	
	$\dd{F(\{a\})}=F(a)-F(a-0)$
	
	$== \frac{1}{2π}φ(a)[F(a)-F(a-0)] +F(b-0)-F(a)+\frac 1{2π}φ(b)[F(b)-F(b-0)]= F(b)-F(a)$.
	\underline{Dernière partie} On suppose $\dd{F(x)} = f(x)\dd{x}$ et $∫_{-∞}^∞ |φ(t)|\dd{t}<+∞$.
	On applique le th président: 
	$F(b)-F(a)=∫_a^bf(x)\dd{x}=∫_a^b\frac 1{2π}$
\end{proof}

\section{Espérances conditionnelles}
1 (as discret)
On a 2 v.a. discrètes:
$X=∑_ix_i\ind_{A_i}, Y=∑_jy_h\ind_{B_j}$
$P_X=∑_ip_iδ_{\{x_i\}}$
$P_Y=∑_jq_jδ_{\{y_j\}}$
\begin{definition}
	On appelle loi de probabilité de $Y$ conditionnelle à $X=x_j$ la quantité suivante:
	$∑_jb_i(j)δ_{\{y_j\}}, où b_i(j)=\frac{\pro(Y=Y_j, X=X_i)}{\pro(X=X_i)}=\frac{Y=Y_j|X=X_1}=\frac{p_{ij}}{p_i}$?
\end{definition}

	Supposons que Y a une espérance finie, c-a-d $\E(|Y|)=∑_j|y_j|q_j<+∞$. Si cette Hypothèse est vraie on a aussi que:
	$$∑_jY_jb_{ij}=∑_jy_j\pro(Y=Y_j|X=X_i)<∞$$
	
	$\E(Y)=∑_jy_hq_j=∑_jy_j\pro(Y=Y_j)$

\begin{notations}
	$\E(Y)(Y|X=x_0)=∑_jy_j\pro(Y|X=x_i)$
	Cette quantité dépend de $\{x_i\}$. Donc cela nous suggère d'introduire une nouvelle v.a. à valeurs
	$\E(Y|X=x_i)$ et poids $p_i=\pro(X=x_i)$
\end{notations}
\begin{definition}
	Cette v.a. qu'on vient de construire et dénotée $\E(Y|X)$ et on l'appelle \textsc{l'Espérance}.
\end{definition}
$$\E_X(\E(Y|X))=∑_i\E(Y|X=x_i)p_i=∑_i∑_jy_jp_{ij}p_i=∑_jY_j(∑_ip_{ij}p_i)=∑_jy_jq_j=\E(Y)$$

\emph{Cas contenu} $X\mapsto f_x(x)$ densité, $Y\mapsto f_Y(y)$ aussi densité.
Rappels: $f_X(x)=∫f_{XY}(x,y)\dd{y}$. 
\begin{definition}
	On définit \textsc{la Densité} conditionnel de $Y$ et sachant la valeur $\{X=x\}$ la fonction de $y$:
	% $$f_{Y|X}(x|y)=\caz{\frac{f_{XY}}{f_X}(x) si f_X(x)≠0 \\ 0 si f_X(x)=0}$$
	L'espérance conditionnelle de $X$ en sachant $\{Y=y\}$
	% $$f_{X|Y}(x|y)=\caz{ \frac{f_{XY}}f_Y(y) si f_Y(y)≠0 \\ 0 si f_X(x)=0 } $$
\end{definition}
Si $f_X(x)≠0$ on a: $f_{XY}(x,y)=f_{Y|X}(y|x)f_X(x)$
Cet égalité vraie aussi si $f_X(x)=0$!
Pourquoi? Supposons que $f_X(x)=0$. donc $f_{XY}(x,y)=0\ \forall$ presque toute $y$.
\begin{definition}
	On appelle \textsc{l'Espérance} conditionnelle de $Y$ en sachant $X$, dénotée $\E(Y|X)$ la variable aléatoire à valeurs $\E(Y|X=x)$ et densité $f_X(x)$ et $\E(Y|X=x)=∫y\underbrace{f_{Y|X}(y|x)}_{\text{espérance conditionnelle de $Y$ en sachant $X=x$}}\dd{y}$.
\end{definition}
\begin{proposition}
	$\E(\E(Y|X))\overset{?}{=}\E(Y)$. Ce vraie.
\end{proposition}
\begin{proof}
	$\E(\E(Y|X))=∫_\R\E(Y|X=x)f_X(x)\dd{x}=∫_\R(∫yf_{Y|X}(y|x)\dd{y})f_X(x)\dd{x}\overset{\text{change?}}{=}∫_\R\dd{y}y(∫f_{Y|X}(y|x)f_X(x)\dd{x})=∫_\R\dd{y}yf_Y(y)=\E(y)$ $∫|y|f_{Y|X}(y)\dd{y}<+∞$ Suffit de découvrir que $∫|y|f_Y(y)<∞$?
	Fubini:
	$\iint_{\R\times\R}f(x,y)\dd{x}\dd{y}$ * si $∫∫|f(x,y)|\dd{x}\dd{y}<∞$ * on peut iterer les intégrales * $si ∫|f(x,y)|\dd{x}<∞$ et $∫\dd{y}∫|f(x,y)|\dd{x}<∞$ alors on peut inter-changer les intégrales. $∫∫_\R \dd{x}\dd{y}|yf_{Y|X}(y|x)f_X(x)|=∫∫_{\R\times\R}\dd{x}\dd{y}|y|f_{XY}(x,y)=∫_\R|y|\dd{y}∫\dd{x}f_{XY}f_{XY}(x,y) Y\in L^1(\pro)$.
\end{proof}

\begin{definition}[Règle de calcul]
	Soient $X$ et $Y$ a densité. Soit $g:\R^2\rightarrow \R$ mesurable et on définit $g(X,Y)$. On définît:
	$\E(g(X,Y)|X)$ de cette manière: $\E(g(X,Y)|X=x)\overbrace{\text{def}}{=}∫g(x,y)f_{X|Y}(x,y)\dd{y}$.
\end{definition}
\begin{definition}
	Soit $A\subset Ω$ une ensemble mesurable dans l'univers $Ω$, et soit $X:Ω\rightarrow \R$ une v.a. à densité $\P(A|X=x)\overset{\text{def}}=\E(\ind_A|X=x)=∫\ind_A(x,y)f_{X|Y}f_{Y|X}(x|y)\dd{y}$.
\end{definition}
\begin{example}
	Calculer la probabilité que $\pro(X<Y)$ $A=\{X<Y\}$ $\pro(X<Y)=\pro(A)$.
\end{example}
\begin{proposition}
	On a:
	\begin{enumerate} 
		\item $\E(\E(g(X,Y)|X))=\E(g(X,Y))$\\
		$\ind_A(X,Y)=g(X,Y)=gº(X,Y)$ $g(x,y)=\ind_{\{x<y\}}(x,y)$ $\pro(A)=\E(\ind_A)=\E(\E(\ind_A|X))$
		\item Si X et Y sont indépendantes $\E(gºY|X)=\E(gºY)$
		\item $\E(gºX|X)=gºX$
		\item $\E[(g_1ºX)(g_2ºY)|X]=g_1ºX\E(g_2ºY|Y)$
	\end{enumerate}
\end{proposition}
\begin{proof}
	$∫(\E(g(X,Y)|X=x))f_X(x)\dd{x}=∫(∫g(x,y)f_{Y|X})\dd{y})f_X(x)\dd{x}=∫\dd{y}(∫g(x,y)f_{XY}(x,y)\dd{x})=\E(g(X,Y))$. Hyp $∫∫|g(x,y)|f_{X,Y}(x,y)<∞$.
\end{proof}

\section{Convergence de variables aléatoires} % (fold)
\label{sec:convergence_de_variables_aleatoires}
Idée On a une suite de v.a. $\{X_n\}_{n≥1}$ définie sur $(Ω,\mathcal{A}, \pro)$. Est-ce que $X_n$ \emph{converge} pour $n\to+∞$?
Il y a plusieurs façon de converger. 

\subsection{Convergence en probabilité} % (fold)
\label{sub:convergence_en_probabilite}
On dira que la suite $X_n$ Converge en Probabilité vers la v.a. $X$ aussi définie sur $(Ω,\mathcal{A})$ et on écrit $X_n\toP X$, si $\forall ε>0$ $$\pro(|X_n-X|>ε)\to 0\ n\to∞$$.
\begin{remark}
	$\pro(ω\in Ω:\ |X_n(ω)-Χ(ω)|>ε)$
\end{remark}
% subsection convergence_en_probabilite (end)
\subsection{Convergence en norme $L^p$} % (fold)
\label{sub:convergence_en_norme_l_p}
\begin{definition}
	On dira que $X_n$ converge en norme $L^p$ vers $X$ et on écrira $X_n\to^{L^p}X$ si
	$$\norm{X-X_n}_p\to 0,\ n\to∞.$$
\end{definition}
\begin{remark}
	$\norm{X}_p=(∫|x|^p\dd{\pro})^{\frac 1p}$.
\end{remark}
\begin{proposition}
	Si $X_n$ converge en norme $L^p$ vers $X$ pour un certain $p$ alors $X_n$ converge en probabilité vers $X$.
\end{proposition}
\begin{proof}
	Est base sur l'inégalité de Chebyshev.
	$\pro(X>ε)≤\frac 1ε\E(X)$
	$\E(X)=∫_ΩΧ\pro=∫_{\{X>ε\}}X\dd{\pro}+∫_{\{X≤ε\}}X\dd{\pro}≥ε\pro(X>ε)$
	On a:
	$\norm{X_n-X}_p\to 0$
	$\pro(|X_n-X|^p>ε^p)≤\frac 1{ε^p}∫|X_n-X|^p\dd{\pro}=\frac 1{ε^p}\norm{X_n-X}_p^p\to 0$.
\end{proof}
% subsection convergence_en_norme_l_p (end)
\paragraph{Convergence presque partout} % (fold)
\label{par:convergence_presque_partout}
On dira que la suite $\{X_n\}_{n≥1}$ converge Presque Partout vers $X$ et on écrira:
$X_n\toPP X$ s'il existe un ensemble $N\in\mathcal{A}$, $\pro$---négligeable ($\pro(N)=0$) tel que, $\forall ω\in N^C$ on a $\lim_{n\to ∞}X_n(ω)=X(ω)$.
% paragraph convergence_presque_partout (end)
% section convergence_de_variables_aleatoires (end)

